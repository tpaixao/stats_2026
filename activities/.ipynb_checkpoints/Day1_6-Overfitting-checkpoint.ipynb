{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c64e071-e10a-403c-8127-d7020aada65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Activity 6: Overfitting - The Complexity vs. Simplicity Trade-off\n",
    "# =============================================================================\n",
    "# \n",
    "# GOAL: Discover how complex models can fit training data perfectly\n",
    "#       but fail miserably on new data (overfitting)\n",
    "#\n",
    "# CONCEPTS: Bias-variance tradeoff, training vs. test error, \n",
    "#           generalization, Occam's Razor\n",
    "#\n",
    "# THE LESSON: More complex is not always better!\n",
    "# =============================================================================\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# PART 1: Generate Data with a Simple True Relationship\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "set.seed(2024)\n",
    "\n",
    "# The TRUE relationship is simple: y = 2 + 3x (linear!)\n",
    "x_train <- seq(0, 10, length.out = 20)\n",
    "y_true <- 2 + 3 * x_train\n",
    "\n",
    "# But our observations have noise (measurement error, biological variability)\n",
    "y_observed <- y_true + rnorm(20, mean = 0, sd = 8)\n",
    "\n",
    "# Plot the data\n",
    "plot(x_train, y_observed, \n",
    "     pch = 16, cex = 1.5,\n",
    "     xlab = \"X\", ylab = \"Y\",\n",
    "     main = \"Training Data\\n(True relationship: y = 2 + 3x)\",\n",
    "     ylim = c(-5, 40))\n",
    "lines(x_train, y_true, col = \"green\", lwd = 3, lty = 2)\n",
    "legend(\"topleft\", \n",
    "       c(\"Observed data\", \"True relationship\"), \n",
    "       pch = c(16, NA), \n",
    "       lty = c(NA, 2),\n",
    "       col = c(\"black\", \"green\"),\n",
    "       lwd = c(NA, 3))\n",
    "\n",
    "cat(\"=================================================================\\n\")\n",
    "cat(\"We have 20 data points with noise.\\n\")\n",
    "cat(\"TRUE relationship: y = 2 + 3x (simple linear)\\n\")\n",
    "cat(\"Let's try fitting models of increasing complexity...\\n\")\n",
    "cat(\"=================================================================\\n\\n\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# PART 2: Model 1 - Too Simple (Underfitting)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Just predict the mean (ignores x completely!)\n",
    "model_mean <- mean(y_observed)\n",
    "\n",
    "plot(x_train, y_observed, \n",
    "     pch = 16, cex = 1.5,\n",
    "     xlab = \"X\", ylab = \"Y\",\n",
    "     main = \"Model 1: Just the Mean (Too Simple!)\",\n",
    "     ylim = c(-5, 40))\n",
    "abline(h = model_mean, col = \"blue\", lwd = 3)\n",
    "lines(x_train, y_true, col = \"green\", lwd = 2, lty = 2)\n",
    "legend(\"topleft\", \n",
    "       c(\"Data\", \"True relationship\", \"Model: mean(y)\"),\n",
    "       pch = c(16, NA, NA),\n",
    "       lty = c(NA, 2, 1),\n",
    "       col = c(\"black\", \"green\", \"blue\"),\n",
    "       lwd = c(NA, 2, 3))\n",
    "\n",
    "# Calculate training error\n",
    "error_mean <- sum((y_observed - model_mean)^2) / length(y_observed)\n",
    "cat(\"Model 1 (constant mean):\\n\")\n",
    "cat(\"Training error (MSE):\", round(error_mean, 2), \"\\n\")\n",
    "cat(\"This model UNDERFITS - too simple to capture the trend!\\n\\n\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# PART 3: Model 2 - Just Right (Good Fit)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Fit a simple linear model (which is actually correct!)\n",
    "model_linear <- lm(y_observed ~ x_train)\n",
    "\n",
    "plot(x_train, y_observed, \n",
    "     pch = 16, cex = 1.5,\n",
    "     xlab = \"X\", ylab = \"Y\",\n",
    "     main = \"Model 2: Linear (Just Right!)\",\n",
    "     ylim = c(-5, 40))\n",
    "abline(model_linear, col = \"red\", lwd = 3)\n",
    "lines(x_train, y_true, col = \"green\", lwd = 2, lty = 2)\n",
    "legend(\"topleft\", \n",
    "       c(\"Data\", \"True relationship\", \"Linear model\"),\n",
    "       pch = c(16, NA, NA),\n",
    "       lty = c(NA, 2, 1),\n",
    "       col = c(\"black\", \"green\", \"red\"),\n",
    "       lwd = c(NA, 2, 3))\n",
    "\n",
    "error_linear <- sum(residuals(model_linear)^2) / length(y_observed)\n",
    "cat(\"Model 2 (linear):\\n\")\n",
    "cat(\"Training error (MSE):\", round(error_linear, 2), \"\\n\")\n",
    "cat(\"Better fit than constant mean!\\n\\n\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# PART 4: Model 3 - Too Complex (Overfitting)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Fit a 10th degree polynomial (way too complex!)\n",
    "# This will fit almost every point perfectly\n",
    "\n",
    "model_poly10 <- lm(y_observed ~ poly(x_train, degree = 10, raw = TRUE))\n",
    "\n",
    "# Generate smooth predictions\n",
    "x_smooth <- seq(0, 10, length.out = 200)\n",
    "y_poly10 <- predict(model_poly10, newdata = data.frame(x_train = x_smooth))\n",
    "\n",
    "plot(x_train, y_observed, \n",
    "     pch = 16, cex = 1.5,\n",
    "     xlab = \"X\", ylab = \"Y\",\n",
    "     main = \"Model 3: 10th Degree Polynomial (Too Complex!)\",\n",
    "     ylim = c(-10, 50))\n",
    "lines(x_smooth, y_poly10, col = \"purple\", lwd = 3)\n",
    "lines(x_train, y_true, col = \"green\", lwd = 2, lty = 2)\n",
    "legend(\"topleft\", \n",
    "       c(\"Data\", \"True relationship\", \"10th degree poly\"),\n",
    "       pch = c(16, NA, NA),\n",
    "       lty = c(NA, 2, 1),\n",
    "       col = c(\"black\", \"green\", \"purple\"),\n",
    "       lwd = c(NA, 2, 3))\n",
    "\n",
    "error_poly10 <- sum(residuals(model_poly10)^2) / length(y_observed)\n",
    "cat(\"Model 3 (10th degree polynomial):\\n\")\n",
    "cat(\"Training error (MSE):\", round(error_poly10, 2), \"\\n\")\n",
    "cat(\"Lowest training error! But look at those wild wiggles...\\n\\n\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# PART 5: Compare Training Errors\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "cat(\"=================================================================\\n\")\n",
    "cat(\"TRAINING ERROR COMPARISON:\\n\")\n",
    "cat(\"=================================================================\\n\")\n",
    "cat(\"Model 1 (mean):       MSE =\", round(error_mean, 2), \"\\n\")\n",
    "cat(\"Model 2 (linear):     MSE =\", round(error_linear, 2), \"\\n\")\n",
    "cat(\"Model 3 (poly 10):    MSE =\", round(error_poly10, 2), \" <- BEST!\\n\\n\")\n",
    "\n",
    "cat(\"On training data, complex model wins!\\n\")\n",
    "cat(\"But wait... let's test on NEW data...\\n\\n\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# PART 6: The TRUE Test - New Data!\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Generate NEW test data from the same true relationship\n",
    "x_test <- seq(0, 10, length.out = 15)\n",
    "y_test_true <- 2 + 3 * x_test\n",
    "y_test_observed <- y_test_true + rnorm(15, mean = 0, sd = 8)\n",
    "\n",
    "# Predictions from each model\n",
    "pred_mean <- rep(model_mean, length(x_test))\n",
    "pred_linear <- predict(model_linear, newdata = data.frame(x_train = x_test))\n",
    "pred_poly10 <- predict(model_poly10, newdata = data.frame(x_train = x_test))\n",
    "\n",
    "# Calculate TEST errors\n",
    "test_error_mean <- sum((y_test_observed - pred_mean)^2) / length(y_test_observed)\n",
    "test_error_linear <- sum((y_test_observed - pred_linear)^2) / length(y_test_observed)\n",
    "test_error_poly10 <- sum((y_test_observed - pred_poly10)^2) / length(y_test_observed)\n",
    "\n",
    "cat(\"=================================================================\\n\")\n",
    "cat(\"TEST ERROR (on new data):\\n\")\n",
    "cat(\"=================================================================\\n\")\n",
    "cat(\"Model 1 (mean):       MSE =\", round(test_error_mean, 2), \"\\n\")\n",
    "cat(\"Model 2 (linear):     MSE =\", round(test_error_linear, 2), \" <- BEST!\\n\")\n",
    "cat(\"Model 3 (poly 10):    MSE =\", round(test_error_poly10, 2), \" <- WORST!\\n\\n\")\n",
    "\n",
    "cat(\"SURPRISE! The complex model that fit training data perfectly\\n\")\n",
    "cat(\"performs TERRIBLY on new data!\\n\\n\")\n",
    "cat(\"This is OVERFITTING.\\n\\n\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# PART 7: Visualize the Disaster\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Show all models on test data\n",
    "plot(x_test, y_test_observed, \n",
    "     pch = 16, cex = 1.5, col = \"darkgreen\",\n",
    "     xlab = \"X\", ylab = \"Y\",\n",
    "     main = \"Predictions on NEW Test Data\",\n",
    "     ylim = c(-50, 80))\n",
    "\n",
    "points(x_test, pred_mean, col = \"blue\", pch = 4, cex = 1.5, lwd = 2)\n",
    "points(x_test, pred_linear, col = \"red\", pch = 3, cex = 1.5, lwd = 2)\n",
    "points(x_test, pred_poly10, col = \"purple\", pch = 2, cex = 1.5, lwd = 2)\n",
    "\n",
    "lines(x_test, y_test_true, col = \"green\", lwd = 3, lty = 2)\n",
    "\n",
    "legend(\"topleft\", \n",
    "       c(\"True test data\", \"True relationship\", \n",
    "         \"Mean model\", \"Linear model\", \"Poly 10 model\"),\n",
    "       pch = c(16, NA, 4, 3, 2),\n",
    "       lty = c(NA, 2, NA, NA, NA),\n",
    "       col = c(\"darkgreen\", \"green\", \"blue\", \"red\", \"purple\"),\n",
    "       lwd = c(NA, 3, 2, 2, 2))\n",
    "\n",
    "cat(\"Notice how the polynomial model makes WILD predictions\\n\")\n",
    "cat(\"outside the training range and even within it!\\n\\n\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# PART 8: The Bias-Variance Tradeoff\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Let's simulate this many times to see the pattern\n",
    "\n",
    "n_simulations <- 100\n",
    "errors_train <- matrix(NA, nrow = n_simulations, ncol = 3)\n",
    "errors_test <- matrix(NA, nrow = n_simulations, ncol = 3)\n",
    "\n",
    "for (i in 1:n_simulations) {\n",
    "  # Generate training data\n",
    "  y_train <- y_true + rnorm(20, sd = 8)\n",
    "  \n",
    "  # Generate test data\n",
    "  y_test <- y_test_true + rnorm(15, sd = 8)\n",
    "  \n",
    "  # Fit models\n",
    "  m1 <- mean(y_train)\n",
    "  m2 <- lm(y_train ~ x_train)\n",
    "  m3 <- lm(y_train ~ poly(x_train, 10, raw = TRUE))\n",
    "  \n",
    "  # Training errors\n",
    "  errors_train[i, 1] <- mean((y_train - m1)^2)\n",
    "  errors_train[i, 2] <- mean(residuals(m2)^2)\n",
    "  errors_train[i, 3] <- mean(residuals(m3)^2)\n",
    "  \n",
    "  # Test errors\n",
    "  p1 <- rep(m1, length(x_test))\n",
    "  p2 <- predict(m2, newdata = data.frame(x_train = x_test))\n",
    "  p3 <- predict(m3, newdata = data.frame(x_train = x_test))\n",
    "  \n",
    "  errors_test[i, 1] <- mean((y_test - p1)^2)\n",
    "  errors_test[i, 2] <- mean((y_test - p2)^2)\n",
    "  errors_test[i, 3] <- mean((y_test - p3)^2)\n",
    "}\n",
    "\n",
    "# Plot the results\n",
    "par(mfrow = c(1, 2))\n",
    "\n",
    "boxplot(errors_train, \n",
    "        names = c(\"Mean\", \"Linear\", \"Poly 10\"),\n",
    "        col = c(\"lightblue\", \"lightcoral\", \"lavender\"),\n",
    "        main = \"Training Error\",\n",
    "        ylab = \"Mean Squared Error\",\n",
    "        ylim = c(0, max(errors_train)))\n",
    "\n",
    "boxplot(errors_test, \n",
    "        names = c(\"Mean\", \"Linear\", \"Poly 10\"),\n",
    "        col = c(\"lightblue\", \"lightcoral\", \"lavender\"),\n",
    "        main = \"Test Error (The Truth!)\",\n",
    "        ylab = \"Mean Squared Error\",\n",
    "        ylim = c(0, max(errors_test)))\n",
    "\n",
    "par(mfrow = c(1, 1))\n",
    "\n",
    "cat(\"=================================================================\\n\")\n",
    "cat(\"AVERAGE ERRORS OVER 100 SIMULATIONS:\\n\")\n",
    "cat(\"=================================================================\\n\\n\")\n",
    "\n",
    "cat(\"Training Error:\\n\")\n",
    "cat(\"Mean model:  \", round(mean(errors_train[, 1]), 2), \"\\n\")\n",
    "cat(\"Linear:      \", round(mean(errors_train[, 2]), 2), \"\\n\")\n",
    "cat(\"Poly 10:     \", round(mean(errors_train[, 3]), 2), \" <- Best on training\\n\\n\")\n",
    "\n",
    "cat(\"Test Error:\\n\")\n",
    "cat(\"Mean model:  \", round(mean(errors_test[, 1]), 2), \"\\n\")\n",
    "cat(\"Linear:      \", round(mean(errors_test[, 2]), 2), \" <- Best on test!\\n\")\n",
    "cat(\"Poly 10:     \", round(mean(errors_test[, 3]), 2), \" <- Disaster!\\n\\n\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# PART 9: YOUR TURN - Find the Right Complexity\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "cat(\"=================================================================\\n\")\n",
    "cat(\"YOUR TURN: What's the right degree polynomial?\\n\")\n",
    "cat(\"=================================================================\\n\\n\")\n",
    "\n",
    "# TODO: Try polynomials of degree 1, 2, 3, 4, 5, ..., 10\n",
    "# For each, calculate training and test error\n",
    "# Plot: degree vs. error\n",
    "\n",
    "degrees <- 1:10\n",
    "train_errors <- numeric(length(degrees))\n",
    "test_errors <- numeric(length(degrees))\n",
    "\n",
    "for (d in degrees) {\n",
    "  # Fit model\n",
    "  if (d == 1) {\n",
    "    model <- lm(y_observed ~ x_train)\n",
    "  } else {\n",
    "    model <- lm(y_observed ~ poly(x_train, degree = d, raw = TRUE))\n",
    "  }\n",
    "  \n",
    "  # Training error\n",
    "  train_errors[d] <- mean(residuals(model)^2)\n",
    "  \n",
    "  # Test error\n",
    "  preds <- predict(model, newdata = data.frame(x_train = x_test))\n",
    "  test_errors[d] <- mean((y_test_observed - preds)^2)\n",
    "}\n",
    "\n",
    "plot(degrees, train_errors, \n",
    "     type = \"b\", col = \"blue\", lwd = 2, pch = 16,\n",
    "     xlab = \"Polynomial Degree (Complexity)\",\n",
    "     ylab = \"Mean Squared Error\",\n",
    "     main = \"The Bias-Variance Tradeoff\",\n",
    "     ylim = c(0, max(c(train_errors, test_errors))))\n",
    "lines(degrees, test_errors, type = \"b\", col = \"red\", lwd = 2, pch = 16)\n",
    "legend(\"topright\", \n",
    "       c(\"Training Error\", \"Test Error\"),\n",
    "       col = c(\"blue\", \"red\"),\n",
    "       lwd = 2, pch = 16)\n",
    "\n",
    "cat(\"Optimal degree (lowest test error):\", degrees[which.min(test_errors)], \"\\n\\n\")\n",
    "\n",
    "cat(\"OBSERVATION:\\n\")\n",
    "cat(\"- Training error ALWAYS decreases with complexity\\n\")\n",
    "cat(\"- Test error has a U-shape: decreases then increases\\n\")\n",
    "cat(\"- Sweet spot is where test error is minimized\\n\")\n",
    "cat(\"- This is the BIAS-VARIANCE TRADEOFF\\n\\n\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# PART 10: Connection to Biology\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "cat(\"=================================================================\\n\")\n",
    "cat(\"WHY THIS MATTERS IN BIOLOGY:\\n\")\n",
    "cat(\"=================================================================\\n\\n\")\n",
    "\n",
    "cat(\"Example 1: Gene Expression Models\\n\")\n",
    "cat(\"- Small sample size (n=20 mice)\\n\")\n",
    "cat(\"- Many potential predictors (20,000 genes!)\\n\")\n",
    "cat(\"- If you fit complex model, you'll overfit to noise\\n\")\n",
    "cat(\"- Need regularization or feature selection\\n\\n\")\n",
    "\n",
    "cat(\"Example 2: Growth Curves\\n\")\n",
    "cat(\"- True relationship might be logistic (sigmoid)\\n\")\n",
    "cat(\"- Fitting 10th degree polynomial overfits\\n\")\n",
    "cat(\"- Better to use mechanistic model (logistic equation)\\n\\n\")\n",
    "\n",
    "cat(\"Example 3: Dose-Response Curves\\n\")\n",
    "cat(\"- Often follow Hill equation (sigmoidal)\\n\")\n",
    "cat(\"- High-degree polynomial might fit perfectly\\n\")\n",
    "cat(\"- But makes nonsense predictions at new doses\\n\\n\")\n",
    "\n",
    "cat(\"LESSON: Use domain knowledge to choose appropriate complexity!\\n\")\n",
    "cat(\"Don't just maximize fit to training data.\\n\\n\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# DISCUSSION QUESTIONS\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "# 1. Why does training error always decrease with model complexity,\n",
    "#    but test error doesn't?\n",
    "#\n",
    "# 2. If you only had the training data (no test set), how would you\n",
    "#    choose the right model complexity? (Hint: cross-validation!)\n",
    "#\n",
    "# 3. In the polynomial example, we knew the true relationship was linear.\n",
    "#    In real biology, we don't know the truth. How do we proceed?\n",
    "#\n",
    "# 4. Occam's Razor says \"prefer simpler models\". But sometimes biology\n",
    "#    IS complex. How do you balance simplicity vs. accuracy?\n",
    "#\n",
    "# 5. What's the connection between overfitting and p-hacking?\n",
    "#    (Both involve fitting to noise!)\n",
    "#\n",
    "# 6. If you have a dataset with n=10 and 100 potential predictors,\n",
    "#    what will happen if you include all 100 in your model?\n",
    "#\n",
    "# ============================================================================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:R]",
   "language": "python",
   "name": "conda-env-R-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
